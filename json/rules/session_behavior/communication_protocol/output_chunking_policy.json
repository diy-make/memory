{
  "name": "output_chunking_policy",
  "description": "Policy for managing output size to prevent token limit overruns and optimize context usage.",
  "single_file_processing": {
    "rule": "When reading a single large file, read from the bottom up. Stop reading when approximately 50% of the available context window is filled. At this point, summarize the information read and explicitly prompt the user for further instructions (e.g., 'What would you like me to do next?').",
    "threshold_guidance": "Approximately 50% of the available context window."
  },
  "multi_file_processing": {
    "rule": "When requested to process a large number of files simultaneously, do not read them all at once. Instead, the agent should create a task for another agent in the swarm to process the files. This is done by sending a message to the swarm with the list of files to be processed. The processing agent will then handle the files in batches, sending updates to the swarm as it makes progress. This allows for iterative processing and prevents context overruns.",
    "batch_size_guidance": "A small number of files that can be summarized within the 50% context threshold.",
    "coordination_note": "Job progress is tracked through swarm communication.",
    "continuous_processing_mode": false
  },
  "path_to_scripting": "This rule can be implemented by a set of scripts and LLM processes that manage the agent's output and context window usage. For `single_file_processing`, a script would read large files from the bottom up, monitoring the remaining context window. When the threshold is met, the script would feed the read content to an LLM for summarization and then prompt the user for instructions. For `multi_file_processing`, a script would, instead of processing all files, create a task message for another agent in the swarm (using the inter-agent communication protocol) containing the list of files to be processed. This promotes distributed processing and prevents context overruns."
}